# -*- coding: utf-8 -*-
"""modelingcode.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1H6BLfe_AzclZTtX81Z5tyguHTHr30XDV
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.gaussian_process import GaussianProcessRegressor, GaussianProcessClassifier
from sklearn.gaussian_process.kernels import RBF, ConstantKernel
from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, classification_report
from sklearn.preprocessing import KBinsDiscretizer
import warnings
warnings.filterwarnings("ignore")

# Load the dataset
df = pd.read_csv('tailor_shop_data.csv')

# Preprocessing
## Handle Dates (Optional, but good practice)
for col in ['Date', 'DueDate']:
    try:
        df[col] = pd.to_datetime(df[col], format='%d-%m-%Y')
    except ValueError:
        print(f"Could not parse dates in column {col}.  Check the format.")

# Data Cleaning and Exploration
print(display(df.head()))
print(display(df.info()))
print(display(df.describe()))

# Visualizing Missing Values
plt.figure(figsize=(10, 6))
sns.heatmap(df.isnull(), yticklabels=False, cbar=False, cmap='viridis')
plt.title('Missing Values Heatmap')
plt.show()

## Impute missing numerical values with median
for col in df.select_dtypes(include=np.number).columns:
    df[col] = df[col].fillna(df[col].median())

## Impute missing categorical values with mode
for col in df.select_dtypes(include='object').columns:
    df[col] = df[col].fillna(df[col].mode()[0])

# --- Feature Importance (Corrected) ---
# Get the one-hot encoded feature names
ohe_feature_names = pipeline.named_steps['preprocessor'].transformers_[1][1].named_steps['onehot'].get_feature_names_out(categorical_features)

# Combine numerical and one-hot encoded feature names
feature_names = numerical_features + list(ohe_feature_names)

# The Pipeline *does not* have a transform method.  Use the preprocessor directly
X_test_clf_processed = pipeline.named_steps['preprocessor'].transform(X_test_clf) # CORRECTED
X_test_clf_df = pd.DataFrame(X_test_clf_processed, columns=feature_names)

# Redefine the feature perturbation importance function to accept a processed DataFrame
def feature_perturbation_importance(model, X, y, feature_names, n_repeats=10):
    original_predictions = model.predict(X)
    original_accuracy = accuracy_score(y, original_predictions)
    importances = {}

    for feature in feature_names:
        importances[feature] = 0.0

        for _ in range(n_repeats):
            X_perturbed = X.copy()
            X_perturbed[feature] = np.random.permutation(X_perturbed[feature].values)

            perturbed_predictions = model.predict(X_perturbed)
            perturbed_accuracy = accuracy_score(y, perturbed_predictions)
            importances[feature] += original_accuracy - perturbed_accuracy

        importances[feature] /= n_repeats

    return importances

# Calculate feature importances using the perturbation method on the *processed* data
#  Pass X_test_clf_df.copy() to avoid modifying the original dataframe

try:
    importances = feature_perturbation_importance(pipeline.named_steps['classifier'], X_test_clf_df.copy(), y_test_clf, feature_names)


    # Sort features by importance
    sorted_importances = sorted(importances.items(), key=lambda x: x[1], reverse=True)

    # Print sorted feature importances in a table format
    print("\nFeature Importances:")
    print("-" * 50)
    print(f"{'Feature':<30} {'Importance':<15}")
    print("-" * 50)
    for feature, importance in sorted_importances:
        print(f"{feature:<30} {importance:<15.5f}")  # Adjust formatting as needed
    print("-" * 50)

    # Plotting feature importances
    plt.figure(figsize=(10, 6))
    feature_names, feature_importances = zip(*sorted_importances)
    plt.barh(feature_names, feature_importances)
    plt.xlabel("Feature Importance")
    plt.ylabel("Feature")
    plt.title("Feature Importances (Gaussian Process Classification)")
    plt.tight_layout()
    plt.show()

except ValueError as e:
    print(f"ValueError in feature importance calculation: {e}")
    print("Check if the feature names are correctly aligned after preprocessing.")
    print("Consider inspecting the output of the ColumnTransformer and OneHotEncoder.")

# --- Gaussian Process Regression ---
## Define features and target variable
numerical_features = ['Chest', 'Arm Length', 'Shoulder', 'Armscye', 'Neck Size', 'Back Width', 'Waist', 'Front Shoulder to Waist', 'Hip', 'Waist to Knee', 'Crotch Depth', 'Body Rise', 'Waist to Floor', 'Nape to Waist', 'Bust']
X_regression = df[numerical_features]
y_regression = df['Cost']

## Split data into training and testing sets
X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X_regression, y_regression, test_size=0.2, random_state=42)

## Scale numerical features
scaler = StandardScaler()
X_train_scaled_reg = scaler.fit_transform(X_train_reg)
X_test_scaled_reg = scaler.transform(X_test_reg)

## Define Gaussian Process Regressor with a kernel
kernel = ConstantKernel(1.0, constant_value_bounds="fixed") * RBF(length_scale=1.0, length_scale_bounds="fixed")
gpr = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10)

## Train the model
gpr.fit(X_train_scaled_reg, y_train_reg)

## Make predictions
y_pred_reg, sigma = gpr.predict(X_test_scaled_reg, return_std=True)

## Evaluate the model
mse = mean_squared_error(y_test_reg, y_pred_reg)
r2 = r2_score(y_test_reg, y_pred_reg)

# Print Gaussian Process Regression Metrics in a table
print("\nGaussian Process Regression Evaluation:")
print("-" * 40)
print(f"{'Metric':<25} {'Value':<10}")
print("-" * 40)
print(f"{'Mean Squared Error (MSE)':<25} {mse:<10.5f}")
print(f"{'R-squared (R^2)':<25} {r2:<10.5f}")
print("-" * 40)

# Plotting GPR results
plt.figure(figsize=(12, 6))
plt.scatter(y_test_reg, y_pred_reg, alpha=0.7)
plt.plot([y_test_reg.min(), y_test_reg.max()], [y_test_reg.min(), y_test_reg.max()], 'k--', lw=2)
plt.xlabel('Actual Cost')
plt.ylabel('Predicted Cost')
plt.title('Actual vs Predicted Cost (Gaussian Process Regression)')
plt.show()

plt.figure(figsize=(12, 6))
plt.errorbar(range(len(y_test_reg)), y_test_reg, yerr=sigma, fmt='o', alpha=0.5)
plt.plot(y_pred_reg, 'r-', label='Predictions')
plt.xlabel('Sample Index')
plt.ylabel('Cost')
plt.title('Predictions with Uncertainty (Gaussian Process Regression)')
plt.legend()
plt.show()

# Residual Plot
plt.figure(figsize=(12, 6))
residuals = y_test_reg - y_pred_reg
plt.scatter(y_pred_reg, residuals, alpha=0.7)
plt.hlines(y=0, xmin=y_pred_reg.min(), xmax=y_pred_reg.max(), colors='red', linestyles='--')
plt.xlabel('Predicted Cost')
plt.ylabel('Residuals')
plt.title('Residual Plot (Gaussian Process Regression)')
plt.show()

# --- Gaussian Process Classification ---

# Binary Classification based on Cost threshold
cost_threshold = df['Cost'].median()
df['Cost_Category'] = df['Cost'].apply(lambda x: 'High' if x > cost_threshold else 'Low')

# Define features and target variable
categorical_features = ['GarmentType', 'Material']
numerical_features = ['Chest', 'Arm Length', 'Shoulder', 'Armscye', 'Neck Size', 'Back Width', 'Waist', 'Front Shoulder to Waist', 'Hip', 'Waist to Knee', 'Crotch Depth', 'Body Rise', 'Waist to Floor', 'Nape to Waist', 'Bust']
X_classification = df[numerical_features + categorical_features]
y_classification = df['Cost_Category']

# Split data into training and testing sets
X_train_clf, X_test_clf, y_train_clf, y_test_clf = train_test_split(X_classification, y_classification, test_size=0.2, random_state=42)

# Preprocessor
numeric_transformer = Pipeline(steps=[('scaler', StandardScaler())])
categorical_transformer = Pipeline(steps=[('onehot', OneHotEncoder(handle_unknown='ignore'))])
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numerical_features),
        ('cat', categorical_transformer, categorical_features)])

# Define Gaussian Process Classifier with a kernel
kernel = RBF(length_scale=1.0)
gpc = GaussianProcessClassifier(kernel=kernel, random_state=42)

# Pipeline for preprocessing and classification
pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('classifier', gpc)])

# Train the model
pipeline.fit(X_train_clf, y_train_clf)

# Make predictions
y_pred_clf = pipeline.predict(X_test_clf)

# Print Gaussian Process Classification Results in a table format
print("\nGaussian Process Classification Evaluation:")
print("-" * 50)
print(f"{'Metric':<30} {'Value':<15}")
print("-" * 50)
print(f"{'Accuracy':<30} {accuracy:<15.5f}")
print("-" * 50)
print("Classification Report:\n", report)
print("-" * 50)

# Confusion Matrix
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test_clf, y_pred_clf, labels=pipeline.classes_)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=pipeline.classes_, yticklabels=pipeline.classes_)
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix (Gaussian Process Classification)')
plt.show()

# Probability Distribution Plot
if hasattr(pipeline.named_steps['classifier'], "predict_proba"):
    probas = pipeline.predict_proba(X_test_clf)
    for i, label in enumerate(pipeline.classes_):
        sns.kdeplot(probas[:, i], label=f'Probability of {label}')
    plt.xlabel('Probability')
    plt.ylabel('Density')
    plt.title('Probability Distribution (Gaussian Process Classification)')
    plt.legend()
    plt.show()

# Stock Plot after Confusion Matrix
num_points = 100
time = np.arange(num_points)
stock_price = np.random.randn(num_points).cumsum()

# Stock Plot based on Predicted vs. Actual
# Aggregate data
results_df = pd.DataFrame({'Actual': y_test_clf, 'Predicted': y_pred_clf})
results_df['Correct'] = results_df['Actual'] == results_df['Predicted']
summary_df = results_df.groupby('Actual')['Correct'].agg(['count', 'sum'])
summary_df['Incorrect'] = summary_df['count'] - summary_df['sum']
summary_df = summary_df.rename(columns={'sum': 'Correct'})

# Prepare data for stock plot
categories = summary_df.index.tolist()
correct_counts = summary_df['Correct'].tolist()
incorrect_counts = summary_df['Incorrect'].tolist()

# Create the stock plot
fig, ax = plt.subplots(figsize=(8, 6))
bar_width = 0.35

r1 = np.arange(len(categories))
r2 = [x + bar_width for x in r1]

ax.bar(r1, correct_counts, color='green', width=bar_width, edgecolor='grey', label='Correct')
ax.bar(r2, incorrect_counts, color='red', width=bar_width, edgecolor='grey', label='Incorrect')

# Add labels, title, and axes ticks
ax.set_xlabel('Cost Category', fontweight='bold')
ax.set_xticks([r + bar_width/2 for r in range(len(categories))])
ax.set_xticklabels(categories)
ax.set_ylabel('Count')
ax.set_title('Classification Results by Cost Category')

# Create legend & Show graphic
ax.legend()
plt.tight_layout()
plt.show()


# --- Triple Top Plot ---

# First, group the data by 'GarmentType' and calculate the average cost
garment_avg_cost = df.groupby('GarmentType')['Cost'].mean().sort_values(ascending=False)

# Select the top 3 GarmentTypes
top_3_garments = garment_avg_cost.head(3)

# Now, create the bar plot
plt.figure(figsize=(10, 6))
top_3_garments.plot(kind='bar', color='skyblue')
plt.title('Top 3 Garment Types by Average Cost')
plt.xlabel('Garment Type')
plt.ylabel('Average Cost')
plt.xticks(rotation=45)  # Rotate the x-axis labels for better readability
plt.grid(axis='y', linestyle='--')  # Add a grid for better visualization
plt.tight_layout()  # Adjust layout to prevent labels from overlapping
plt.show()

# Create the stock plot
plt.figure(figsize=(12, 6))
plt.plot(time, stock_price, marker='o', linestyle='-', color='g', label='Stock Price')
plt.xlabel('Time')
plt.ylabel('Stock Price')
plt.title('Stock Price Over Time')
plt.grid(True)  # Show gridlines
plt.legend()      # Show legend
plt.tight_layout()  # Adjust layout to prevent labels from overlapping
plt.show()

